\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage}
\usepackage{algorithm,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4}{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{1}{Sep 06, 2016}{Introduction}{Ambuj Tewari}

\section{Sequential Decision Making in mHealth}

This course will focus on the use of sequential decision making algorithms in mobile health applications to enable long term health behavior change and maintenance.
Behavior change can lead to improved health outcomes in many health conditions including cardiovascular diseases, substance abuse, and mental illnesses.
The promise of mobile health is that tailored interventions can be delivered to users at the right time, in the right context, as they go about living their daily lives.

The basic sequential decision making protocol in mobile health is as follows.

\renewcommand{\algorithmicfor}{\textbf{at}}
\renewcommand{\algorithmicendfor}{\textbf{done}}
\begin{algorithmic}[1]
\FOR{a given decision point}
\STATE mobile phone collects tailoring variables (the context) 
\STATE a decision rule (or policy) maps the tailoring variables into an intervention option (the action) 
\STATE mobile phone records the proximal outcome (interpreted as a reward, so higher is better) 
\ENDFOR
\end{algorithmic}
\renewcommand{\algorithmicfor}{\textbf{for}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}

For example, in a project called HeartSteps that aims to maintain physical activity in user after cardiac rehab, context variables include GPS location, outside weather, label (such as walking, running, in vehicle)
from google activity recognition, calendar busyness. There are two intervention options for momentary suggestions that are delivered on the smartphone's lock screen: $1$, i.e., provide a suggestion and 
$0$, i.e., don't provide a suggestion. The proximal outcome or reward is the number of steps walked (measured via a wristband) in a specified time period, say an hour, following the suggestion.

In this course we will look at increasingly sophisticated methods to sequentially choose actions to maximize sum of received rewards. The first class of methods, called {\em multi-armed bandits}, apply in situations
where there is no contextual information available to guide action selection. This is not a realistic case in mHealth application but serves as a foundation for more sophisticated methods. 
The second class of methods, called {\em contextual bandits}, do take into account contextual information but do not reason about delayed effects of actions on future contexts. Delayed effects can easily
occur in mHealth applications. E.g., providing a too many suggestions to the user can affect the burden involved in using the app and reduce effectiveness of future suggestions since they might be ignored by the user.
Finally we will look at general {\em reinforcement learning} algorithms that do take into account delayed effects.

\section{Multiarmed Bandits: Regret and Basic Definitions}

The name ``bandits" comes from ``one-armed bandit" which refers a slot machine in casinos operated by pulling a long arm at the side. These machines rob you of your money and hence the name ``bandits"!
Now imagine having several arms to choose from. You do not know which arms are good and which ones are bad. So you will have \emph{explore} new arms in the hope of finding good ones that have not been used much
so far but also \emph{exploit} your knowledge from previous rounds to stick to ones that have performed well in the past.

Robbins \cite{robbins1952some} originally formalized the problem that we now refer to as multiarmed bandit. We will use ``action" and ``arm" interchangeably. He considered the case of two actions
but it is not too difficult to extend the framework to consider a set of $K$ actions $\actions = \{0,1,\ldots,K-1\}$. To each action $a \in \actions$, we attach a distribution $D_a$. Assume that $D_a$ has finite mean $\mu_a$ and
consider a sequence of iid random variables
\[
R^a_1,R^a_2,\ldots
\]
distributed according to $D_a$.

The multiarmed bandit (MAB) protocol is as follows.

\begin{algorithmic}[1]
\FOR{$t=1$ to $T$}
\STATE Learner selects action $A_t \in \actions$
\STATE Learner receives reward $R_t = R^{A_t}_t$
\ENDFOR
\end{algorithmic}

A deterministic learning algorithm $\la$ for MAB problems is a sequence of maps $\la_t$ that map the history at time $\history_t = (A_1,R_1,\ldots,A_{t-1},R_{t-1})$ to an action in $\actions$, i.e.,
$A_t = \la_t(\history_t)$. A randomized learning algorithm can use its own private source of randomness in action selection.

The expected regret of a learning algorithm $\la$ against distributions $(D_a)_{a \in \actions}$ is defined as
\begin{equation}\label{eq:mabregret}
\regret_T(\la, (D_a)_{a \in \actions}) := \mu_\star T - \E{\sum_{t=1}^T R_t}
\end{equation}
where $\mu_\star = \max_{a \in \actions} \mu_a$ is the maximum mean reward among all actions. Note that the expectation in the second term is with respect to two sources of randomness.
First, the underlying rewards $R^a_t$ are random. Second, the learning algorithm itself can itself be randomized.

There is another expression for the regret that will be convenient to have later. Define the indicator variable $I_{t,a}$ that turns on iff arm $a$ is selected at time $t$. Define the action counts
\[
N_t(a) = \sum_{s=1}^t I_{s,a} .
\]
We then have,
\begin{equation}\label{eq:mabregret2}
\regret_T(\la, (D_a)_{a \in \actions}) = \sum_{a \in \actions} \Delta_a \E{N_T(a)} = \sum_{a: \mu_a < \mu_\star} \Delta_a \E{N_T(a)}
\end{equation}
where $\Delta_a = \mu_\star - \mu_a$ is the gap between the mean reward of the best action and action $a$.

\emph{Distribution specific} regret bounds are allowed to depend on properties of the individual reward distributions $D_a$ such as the gaps $\Delta_a$.

To derive \emph{distribution free} regret bounds, we think of the distributions $D_a$ as coming from a parametric (e.g., Bernoulli distributions for binary valued rewards or Gaussian distributions for real valued rewards) or a nonparametric family
(e.g., all distributions with support in $[0,1]$) denoted by $\distfam$. We can assess how ``good'' a learning algorithm $\la$ is by looking at its worst possible regret over distributions $D_a$:
\[
\regret_T(\la, \distfam) = \sup_{D_a \in \distfam} \regret_T (\la, (D_a)_{a \in \actions}) .
\]
A bound on $\regret_T(\la, \distfam)$ is a distribution free regret bound that holds no matter which reward distributions are faced by the learning algorithm as long as they are all in the family $\distfam$.

A learning algorithm is called \emph{consistent} for a class $\distfam$ of distributions if, for all $D_a \in \distfam$,
\[
\lim_{T\to \infty} \frac{\regret_T(\la, (D_a)_{a \in \actions})}{T} = 0 .
\]

Finally, the minimax regret against a class $\distfam$ of reward distributions is given by
\[
\regret_T(\distfam) = \inf_{\la} \regret_T(\la, \distfam) .
\]

Determination of exact minimax rates is hard: we usually prove upper and lower bounds separately and try to make them match as much as we can.
Upper bounds on the minimax regret can be established by proving a distribution free regret bound for a specific learning algorithm $\la$. Lower bounds have to reason about all 
possible learning algorithms and usually rely on information theoretic arguments.

\section{A General Consistency Result}

Robbins \cite{robbins1952some} proved a general consistency result for two armed bandit problems that just relies on law of large numbers and easily extends to the case of $K$ arms.
Let $\distfam$ be all distributions with a finite mean.

Consider the following learning algorithm that is referred to as ``certainty equivalence with forcing'' in the control literature. Pre-select an infinite sequence of ``forced exploration" time points
\[
\tau_{a,1} < \tau_{a,2} < \ldots
\]
such that the time points are disjoint over $a \in \actions$ and $t_{a,1} = a-1$ (i.e., we start by selecting each action from $0$ through $K-1$ once). At any given time point $t$, if it one
of the forced exploration time points for an action $a$, select action $a$. Otherwise, form estimates $\hat{\mu}_{t-1,a}$ of $\mu_a$ using the sample mean over all previous exploration rounds for action $a$, and select the
one with maximum value of $\hat{\mu}_{t-1,a}$ (breaking ties, say, in favor of the numerically smaller action).

Define the indicator variables $I'_{t,a}, I''_{t,a}$ to be on iff action $a$ was selected at time $t$ because of exploration, exploitation reasons respectively. Define the counts $N'_t(a), N''_t(a)$ in the same way as we defined
$N_t(a)$. Note that $N_t(a) = N'_t(a) + N''_t(a)$. From~\eqref{eq:mabregret2}, we have
\begin{align*}
\regret_T(\la, (D_a)_{a \in \actions}) &= \sum_{a: \mu_a < \mu_\star} \Delta_a \E{N_T(a)} \\
&= \sum_{a: \mu_a < \mu_\star} \Delta_a \E{N'_T(a)} + \sum_{a: \mu_a < \mu_\star} \Delta_a \E{N''_T(a)} 
\end{align*}
Note that
\[
N'_T(a) = |\left\{ \tau_{a,i} \::\: 1 \le \tau_{a,i} \le T \right\}|
\]
is deterministic. Let us make the assumption that the sequence of exploration points have density zero, i.e., $N'_T(a)/T \to 0$. Then, we just need to ensure that, for all $a \in \actions$,
\[
\frac{ \E{N''_T(a)} }{T} \to 0 .
\]

Now note that we can write the previous quantity as a Cesaro sum:
\[
\frac{ \E{N''_T(a)} }{T} = \frac{\sum_{t=1}^T \E{I''_{t,a}}} {T} .
\]
We know that, if a sequence converges to $0$, then so does its Cesaro sum. So the only thing that remains to be shown is that $\E{I''_{t,a}} \to 0$ as $t \to \infty$ for any 
strictly suboptimal action $a$. If action $a$ is selected as a result of exploitation at time $t$, it must be that $\hat{\mu}_{t-1,a} \ge \hat{\mu}_{t-1,a^\star}$ for any optimal
action $a^\star$. But this can only occur if either $|\hat{\mu}_{t-1,a} - \mu_a| \ge \Delta_a/2$ or $|\hat{\mu}_{t-1,a^\star} - \mu_{a^\star}| \ge \Delta_a/2$. Therefore,
\[
\E{I''_{t,a}} \le \prob{|\hat{\mu}_{t-1,a} - \mu_a| \ge \Delta_a/2} + \prob{|\hat{\mu}_{t-1,a^\star} - \mu_{a^\star}| \ge \Delta_a/2} .
\]
Both terms on the RHS converge to zero by the law of large numbers.

\bibliography{stats710}

\end{document}


