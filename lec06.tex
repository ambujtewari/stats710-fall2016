\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage}
\usepackage{algorithm,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4 }{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{6}{Sep 22, 2016}{Proof of The Upper Bound for \\Thompson Sampling with Beta Priors and Bernoulli Rewards}{Young Jung}
\section{Thompson Sampling with Beta Priors and Bernoulli Rewards}
\subsection{Algorithm Description And Theorem Statement}
Recall from last lecture:
\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE Using beta distribution as prior.
		\STATE For each arm a $\in$ $\actions$, $S_{a}=F_{a}=0$.
		\FOR{t=1,2, $\cdots$,}
		\STATE For each a$\in\actions$, sample $\theta_{a,t} \sim Beta(S_{a}+1,F_{a}+1)$.
		\STATE Play arm $A_t$ = $\argmax_{a\in \actions}$ $\theta_{a,t}$ and collect reward $R_t$.
		\STATE If $R_t=1$, $S_{A_t}=S_{A_t}+1$. Else if $R_t=0$, $F_{A_t}=F_{A_t}+1$.
		\ENDFOR 
		\end{algorithmic}
		\caption{Thompson Sampling for Bernoulli Bandits}
\end{algorithm}

\noindent This algorithm dates back to 1933 \cite{thompson1933likelihood}, but the first finite time regret analysis appeared only in 2012 \cite{agrawal2012analysis}. Our presentation of TS and its proof will follow a later paper \cite{agrawal2013further}. 
\begin{theorem}
	Assume Bernoulli reward distributions for the $K$ arms with means $\mu_0,\cdots,\mu_{K-1}$, where $\mu_0 > max_{a \geq 1}\mu_a$. Then for any $\varepsilon$, 
	\begin{align*}
	R_T(\text{TS with Beta},(\distfam_a)_{a\in\actions} ) \leq (1+\varepsilon)\sum_{a=1}^{K-1}\frac{\log{T}}{d(\mu_a,\mu_0)}\Delta_a +O(\frac{K}{\varepsilon^2})
	\end{align*}
	where $d(\mu_a, \mu_0)$ := $\mu_a \log{\frac{\mu_a}{\mu_0}}+(1-\mu_a)\log{\frac{1-\mu_a}{1-\mu_0}}$.
\end{theorem}
\noindent \textbf{Note}: By Pinsker's inequality, $d(\mu_a, \mu_0)$
$\geq$ $2(\mu_a - \mu_0)^2=2\Delta_a^2$. The upper bound for UCB is
worse than that of TS due to this inequality. 

\subsection{Notation and comments}

\begin{itemize}
    \item $\theta_{a, t}$: draw from arm $a$'s posterior at time $t$. 
    \item $S_{a, t}$: the number of successes (i.e., ones) of arm $a$
      up to and including time $t$.
    \item $F_{a, t}$: the number of failures (i.e., zeroes) of arm $a$
      up to and including time $t$. 
    \item Note that $S_{a, t} + F_{a, t} = N_t(a)$, which is the
      number of times that arm $a$ is selected up to and including time $t$. 
    \item $\hat{\mu}_{a, t-1} = \frac{\sum_{i=1, A_i = a}^{t-1}R_i^a}{N_{t-1}(a)+1} = \frac{S_{a, t-1}}{N_{t-1}(a) + 1}$
    \begin{itemize}
        \item Note that 1 is added to the denominator on purpose to prevent division by zero. 
    \end{itemize}
    \item For each $a$ and any $\varepsilon>0$, we will introduce thresholds $x_a, y_a$ satisfying following conditions. 
    \begin{itemize}
        \item $\mu_a < x_a < y_a < \mu_0$
        \item $d(x_a, \mu_0) = \frac{d(\mu_a, \mu_0)}{1 + \varepsilon}$
        \item $d(x_a, y_a) = \frac{d(x_a, \mu_0)}{1 + \varepsilon} = \frac{d(\mu_a, \mu_0)}{(1 + \varepsilon)^2}$
        \item Note that we can always choose such thresholds due to the continuity of KL-divergence. 
    \end{itemize}
    \item $\mathcal{H}_t = \{A_1, R_1, \cdots, A_{t-1}, R_{t-1}\}$ history of actions and rewards \textit{before} time $t$. 
    \begin{itemize}
        \item For consistency, we use $t$ for the subscript instead of $t-1$. 
    \end{itemize}
    \item We introduce two events. 
    \begin{itemize}
        \item $E^\mu_{a, t} = \{\hat{\mu}_{a, t-1} \leq x_a\}$
        \item $E^\theta_{a, t} = \{\theta_{a, t} \leq y_a\}$
        \item The idea is that these two events are more likely to
          happen as time progresses.
        \item Note that given $\mathcal{H}_t$, $E^\mu_{a, t}$ is determined to be either true of false and $E^\theta_{a, t}$ is still random, but the probability that it happens can be exactly calculated. 
    \end{itemize}
    \item $p_{a, t} := \mathbb{P}(\theta_{0, t} > y_a | \mathcal{H}_t)$
\end{itemize}

\subsection{Key Lemma}
\begin{lemma}
For all $t$ and $a \neq 0$, 
$$
    \mathbb{P}(A_t = a, E^\mu_{a, t}, E^\theta_{a, t} | \mathcal{H}_t) \leq \frac{1 - p_{a, t}}{p_{a, t}} \mathbb{P}(A_t = 0, E^\mu_{a, t}, E^\theta_{a, t} | \mathcal{H}_t)
$$
\end{lemma}

\noindent \textbf{Note}: We expect $\frac{1 - p_{a, t}}{p_{a, t}}$ to be very small. 

\begin{proof}
    Note that $E^\mu_{a, t}$ is fixed given $\mathcal{H}_t$ and there is nothing to prove when it does not happen. From now on, we will assume this event happened. Using Bayes rule, we get:
    \begin{align*}
        \text{LHS} &= \mathbb{P}(A_t = a, E^\theta_{a, t} | \mathcal{H}_t) \\
        &= \mathbb{P}(A_t = a | E^\theta_{a, t}, \mathcal{H}_t) \mathbb{P}(E^\theta_{a, t} | \mathcal{H}_t) \\
        \text{RHS} &= \frac{1 - p_{a, t}}{p_{a, t}} \mathbb{P}(A_t = 0| E^\theta_{a, t}, \mathcal{H}_t) \mathbb{P}(E^\theta_{a, t} | \mathcal{H}_t).
    \end{align*}
    Thus it suffices to prove that $\mathbb{P}(A_t = a | E^\theta_{a, t}, \mathcal{H}_t) \leq \frac{1 - p_{a, t}}{p_{a, t}} \mathbb{P}(A_t = 0| E^\theta_{a, t}, \mathcal{H}_t)$. 
    Given $E^\theta_{a, t}$, $\{A_t = a\}$ implies that $\theta_{\tilde{a}, t} \leq y_a$ for any $\tilde{a} \in \actions$. Therefore, 
    \begin{align*}
        \text{LHS} &\leq \mathbb{P}(\forall \tilde{a}\in \actions, \theta_{\tilde{a}, t} \leq y_a | E^\theta_{a, t}, \mathcal{H}_t) \\ 
        &= \mathbb{P}(\theta_{0, t} \leq y_a | E^\theta_{a, t}, \mathcal{H}_t)\mathbb{P}(\forall \tilde{a} \neq 0, \theta_{\tilde{a}, t} \leq y_a | E^\theta_{a, t}, \mathcal{H}_t) \\
        &= (1 - p_{a, t})\mathbb{P}(\forall \tilde{a} \neq 0, \theta_{\tilde{a}, t} \leq y_a | E^\theta_{a, t}, \mathcal{H}_t)
    \end{align*}
    The last equality holds because the event $\{\theta_{a, t} \leq y_a\}$ is independent of $E^\theta_{a, t}$. Similarly, given $E^\theta_{a, t}$, $\{A_t = 0\}$ happens whenever $\theta_{0, t} > y_a$ and $\theta_{\tilde{a}, t} \leq y_a$ for any $\tilde{a} \neq 0$. This gives us
    \begin{align*}
        \text{RHS} &\geq \frac{1 - p_{a, t}}{p_{a, t}} \mathbb{P}(\theta_{0, t} > y_a, \theta_{\tilde{a}, t} \leq y_a \forall \tilde{a} \neq 0 | E^\theta_{a, t}, \mathcal{H}_t) \\
        &= (1- p_{a, t}) \mathbb{P}(\forall \tilde{a} \neq 0, \theta_{\tilde{a}, t} \leq y_a | E^\theta_{a, t}, \mathcal{H}_t)
    \end{align*}
    The above two inequalities complete the proof of the key lemma.
\end{proof}
		
\subsection{(Incomplete) Proof of Theorem}
We will start by rewriting $N_T(a)$ as the sum of indicator variables
and taking expectation. 
\begin{align*}
    \mathbb{E}N_T(a) &= \sum_{t=1}^T \mathbb{P}(A_t = a) \\
    &= \sum_{t=1}^T [\mathbb{P}(A_t = a, E^\mu_{a, t}, E^\theta_{a, t}) + \mathbb{P}(A_t = a, E^\mu_{a, t}, \overline{E^\theta_{a, t}}) + \mathbb{P}(A_t = a, \overline{E^\mu_{a, t}})] \\
    &= S_1 + S_2 + S_3
\end{align*}

In this (incomplete) proof, we will focus on $S_1$ only. We will control $S_2
$ and $S_2$ in the next lecture. To impose an upper bound, we will use
the tower property of expectation several times. 

\begin{align*}
    \mathbb{P}(A_t = a, E^\mu_{a, t}, E^\theta_{a, t}) &= \mathbb{E}[\mathbb{P}(A_t = a, E^\mu_{a, t}, E^\theta_{a, t}|\mathcal{H}_t)] (\because \text{tower property})\\
    &\leq \mathbb{E}[\frac{1 - p_{a, t}}{p_{a, t}}\mathbb{P}(A_t = 0, E^\mu_{a, t}, E^\theta_{a, t}|\mathcal{H}_t)] (\because \text{key lemma}) \\
    &= \mathbb{E}[\mathbb{E}[\frac{1 - p_{a, t}}{p_{a, t}}\mathbb{I}(A_t = 0, E^\mu_{a, t}, E^\theta_{a, t}|\mathcal{H}_t)]] (\because p_{a,t} \text{ is a function of } \mathcal{H}_t) \\
    &= \mathbb{E}[\frac{1 - p_{a, t}}{p_{a, t}}\mathbb{I}(A_t = 0, E^\mu_{a, t}, E^\theta_{a, t})] (\because \text{tower property})\\
    &\leq \mathbb{E}[\frac{1 - p_{a, t}}{p_{a, t}}\mathbb{I}(A_t = 0)]
\end{align*}

Let $\tau_k$ be the time at which action $0$ is taken for the $k^{th}$ time with $\tau_0 :=0$. The key idea is that $p_{a, t}$ is updated only when $\{A_t = 0\}$ happens. 

\begin{align*}
    S_1 &\leq \sum_{t=1}^T\mathbb{E}[\frac{1 - p_{a, t}}{p_{a, t}}\mathbb{I}(A_t = 0)] \\
    &\leq \mathbb{E}[\sum_{k=0}^{T-1} \frac{1 - p_{a, \tau_k + 1}}{p_{a, \tau_k +1}}\sum_{t=\tau_k +1}^{\tau_{k+1}}\mathbb{I}[A_t = 0]] \\
    &=\mathbb{E}[\sum_{k=0}^{T-1} \frac{1 - p_{a, \tau_k + 1}}{p_{a, \tau_k +1}}]
\end{align*}

The second inequality might not be equality because $k$ can be strictly less than $T-1$. The last equality holds because the second summation from the second line is exactly 1 by definition of $\tau_k$. 

The lecture ended by stating the following lemma without proof. The
proof requires extensive calculations involving Beta and Binomial
distributions. See the appendix of \cite{agrawal2013further} for details. 

\begin{lemma}
\begin{align*}
    \mathbb{E} [\frac{1 - p_{a, \tau_k + 1}}{p_{a, \tau_k +1}} ] \leq 
    \begin{cases}
    \frac{3}{\Delta_a'} &\text{if } k < \frac{8}{\Delta_a'} \\
    \Theta(e^{-k\Delta_a'^2/2} + \frac{1}{(k+1)\Delta_a'^2}e^{-kD_a} + \frac{1}{e^{k\Delta_a'^2/4} -1}) & \text{otherwise}
    \end{cases}
\end{align*}
where $\Delta_a' = \mu_0 - y_a$ and $D_a = d(y_a, \mu_0)$.
\end{lemma}
		
\bibliography{stats710}
\end{document} 