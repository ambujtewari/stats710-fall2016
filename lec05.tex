\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage}
\usepackage{algorithm,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4 }{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{5}{Sep 20, 2016}{Distribution Independent Bounds and \\Thompson Sampling with Beta Priors and Bernoulli Rewards}{Hyesun Yoo}
\section{Small detour into distribution-independent bounds}
\subsection{Upper bound}
We want to know what happens when the gap $\Delta_a$ goes to zero. 
Suppose $\E{N_T(a)} \leq c\frac{\log T}{\Delta_a^2}$. It seems like as
$\Delta_a \to 0$, this bound would blow up. However, we
can derive distribution-independent bounds that do not depend on $\Delta_a$. For any threshold $\delta>0$,\\
\begin{align*}
\text{expected regret} &= \sum_{a} \Delta_a \E{N_T(a)}\\
&= \sum_{a: \Delta_a \leq \delta} \Delta_a  \E{N_T(a)} + \sum_{a: \Delta_a > \delta} \Delta_a \E{N_T(a)}\\
&\leq \delta \cdot \E{\sum_{a: \Delta_a \leq \delta} N_T(a)} + \sum_{a: \Delta_a > \delta} \Delta_a  c\frac{\log T}{\Delta_a^2}\\
&\leq \delta T + \frac{1}{\delta}K c \log{T}
\end{align*}
We don't have $\delta$ in our algorithm so we can tune it. If we set
$\delta = \sqrt{\frac{cK\log{T}}{T}}$, which makes both terms equal,
and plug in, then we would get
\begin{align*}
  \text{expected regret} &\le 2\sqrt{cKT\log{T}}.
\end{align*}

\subsection{Lower bound}
\begin{theorem}  
	Fix any learning algorithm, K and T. Then, there exists a MAB
        problem with Bernoulli rewards such that the expected regret
        is at least $c'\sqrt{KT}$ for some universal constant c$'$.
\end{theorem}

\begin{proof}
\textbf{(informal)}. With $n$ iid samples, we cannot estimate the mean
to an accuracy better than $\frac{1}{\sqrt{n}}$. Since there must be
an arm played at most $\frac{T}{K}$ times, it will be difficult to
distinguish the quality of this arm from any other one whose mean is
within $\frac{1}{\sqrt{T/K}}$. For example, consider a $K$-armed
bandit problem. Suppose there are $K-1$ Bernoulli($1/2$) arms and one
Bernoulli($\frac{1}{2}+\sqrt{\frac{K}{T}}$) arm. You can see how the lower
bound works here. The learning 
algorithm will not be able to figure out which arm is optimal and will suffer regret $T \times \sqrt{\frac{K}{T}} = \sqrt{TK}$.
\end{proof}
 The actual proof is much longer than the proof sketch given
here. The proof of this theorem can be found in
\cite{auer2002nonstochastic} (see the theorem on p. 13 and its proof
on pp. 31-33). We also discussed the lower bound in \cite{lai1985asymptotically} where they used idea of hypothesis testing. Also, there was discussion on the use of KL divergence. If you want to control how random quantities associated with the learning algorithm behave under a change of measure, KL divergence is  a good tool for that.

\section{Thompson sampling}
\subsection{Algorithms}
The beta distribution is useful for Bernoulli rewards because, in a
Bayesian framework, if the prior is $\text{Beta}(\alpha,\beta)$, then
after a single Bernoulli trial, the posterior is either
$\text{Beta}(\alpha+1,\beta)$ or $\text{Beta}(\alpha, \beta+1)$.
% (i.e. pdf of beta: $f(x,\alpha, \beta)$ =
% $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$$x^{\alpha-1}(1-x)^{\beta-1}$.) 
Thompson sampling (with Bernoulli rewards) keeps track of the number of
``successes'', $S_a$, and ``failures'', $F_a$, for each arm and
updates a beta distribution for the selected arm in each round. Algorithm 2 is
a general version of Algorithm 1 in which the rewards are generated
from unknown distributions with support $[0,1]$.
\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE Using beta distribution as prior.
		\STATE For each arm a $\in$ $\actions$, $S_{a}=F_{a}=0$.
		\FOR{t=1,2, $\cdots$,}
		\STATE For each a$\in\actions$, sample $\theta_{a,t} \sim Beta(S_{a}+1,F_{a}+1)$.
		\STATE Play arm $A_t$ = $\argmax_{a\in \actions}$ $\theta_{a,t}$ and collect reward $R_t$.
		\STATE If $R_t=1$, $S_{A_t}=S_{A_t}+1$. Else if $R_t=0$, $F_{A_t}=F_{A_t}+1$.
		\ENDFOR 
		\end{algorithmic}
		\caption{Thompson Sampling for Bernoulli Bandits}
\end{algorithm}

\begin{algorithm}
	\begin{algorithmic}[1]
		\STATE Using beta distribution as prior.
		\STATE For each arm a $\in$ $\actions$, $S_{a}=F_{a}=0$.
		\FOR{t=1,2, $\cdots$,}
		\STATE For each a$\in\actions$, sample $\theta_{a,t} \sim Beta(S_{a}+1,F_{a}+1)$.
		\STATE Play arm $A_t$ = $\argmax_{a\in \actions}$ $\theta_{a,t}$ and collect reward $\tilde{R}_t$ ($\tilde{R}_t$ $\in$ [0,1])
		\STATE \textbf{Perform a Bernoulli trial with success probability} $\tilde{R}_t$ $\textbf{and observe output}$ $R_t$.
		\STATE If $R_t=1$, $S_{A_t}=S_{A_t}+1$. Else if $R_t=0$, $F_{A_t}=F_{A_t}+1$.
		\ENDFOR 
	\end{algorithmic}
	\caption{Thompson Sampling for General Stochastic Bandits}
\end{algorithm}
Comments:
\begin{itemize}
	\item Note that $S_{a,t}$ and $F_{a,t}$ depend on time, but we
          suppressed the subscript $t$.
	\item For exploration, UCB  uses an explicit formula while
          Thompson sampling seems to explore using its posterior
          distribution. The search for unifying principles behind UCB
          and TS algorithms continues. See, e.g., the work of Russo
          and Van Roy \cite{russo2014learning}. 
        \item As you
          get more samples, the posterior distributions for suboptimal arms will be eventually get separated from those for the optimal arm.
	\item If we simply use the posterior mean instead of drawing a
          sample from the posterior distribution, we might not explore
          enough. 
\end{itemize}

\subsection{Theorem for Thompson Sampling}
This algorithm dates back to 1933 \cite{thompson1933likelihood} but the first finite time regret analysis appeared only in 2012 \cite{agrawal2012analysis}. Our presentation of TS and its proof will follow a later paper \cite{agrawal2013further}.
\begin{theorem}
Assume Bernoulli reward distributions for the $K$ arms with means $\mu_0,\cdots,\mu_{K-1}$, where $\mu_0 > max_{a \geq 1}\mu_a$. Then for any $\varepsilon$,
	\begin{align*}
	R_T(\text{TS with Beta},(\distfam)_{a\in\actions} ) \leq (1+\varepsilon)\sum_{a=1}^{K-1}\frac{\log{T}}{d(\mu_a,\mu_0)}\Delta_a +O(\frac{K}{\varepsilon^2})
	\end{align*}
	where $d(\mu_a, \mu_0)$ := $\mu_a \log{\frac{\mu_a}{\mu_0}}+(1-\mu_a)\log{\frac{1-\mu_a}{1-\mu_0}}$.
\end{theorem}

\noindent Comments:
\begin{itemize}
\item Note that by Pinsker's inequality, $d(\mu_a, \mu_0) \geq$ $2(\mu_a - \mu_0)^2=2\Delta_a^2$.
\item The last term in this inequality actually involves $\mu$ and $\Delta_a$, which make our bound complicated.
\item $d(\mu_a,\mu_0)$ can blow up if the optimal arm is
  deterministic, i.e. equal to zero or one with probability
          one. 
	\item At the end of class, there was a discussion on KL-UCB \cite{garivier2011kl} whose finite time bound matches, up to constants, the asymptotic lower bound of Lai and Robbins \cite{lai1985asymptotically}
\end{itemize}
		
\bibliography{stats710}
\end{document} 
