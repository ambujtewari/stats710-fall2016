\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage}
\usepackage{algorithm,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4}{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{7}{Sep 27, 2016}{Finishing the proof of the regret bound for Thompson Sampling with Beta Priors and Bernoulli Rewards}{Yutong Wang}
    \section{Proof of theorem 1 from lecture 6 continued} 
We adopt all notations from the previous lecture. Recall that we left off at
\begin{align*}\E{N_T(a)} &= \sum_{t=1}^T \prob{A_t = a,E^{\mu}_{a,t}, E^{\theta}_{a,t}} + \prob{A_t = a, E^{\mu}_{a,t}, \overline{E^{\theta}_{a,t}}} + \prob{A_t = a, \overline{E^{\mu}_{a,t}}}\\
& = S_1 + S_2 + S_3.
\end{align*}
\subsection{Bounding $S_1$}
Last lecture gave a bound for $S_1$. Summarizing, we had
$$S_1 \le \E{\sum_{k=0}^{T-1}\frac{1- p_{a,\tau_{k + 1}}}{p_{a,\tau_{k+1}}}}$$
and the following lemma
\begin{lemma}\label{A}
$$\E{\frac{1-p_{a,\tau_{k + 1}}}{p_{a,\tau_{k+1}}}} \le \begin{cases}
\frac{3}{\Delta_a'} & \mbox{ if } k < \frac{8}{\Delta_a'}\\
\Theta\left(e^{-k\Delta_a'^2/2}+ \frac{1}{(k+1)\Delta_a'^2}e^{-kD_a}+ \frac{1}{e^{k\Delta_a'^2/4}-1}\right) &\mbox{otherwise}
\end{cases}
$$
where $\Delta_a' = \mu_0 - y_a$ and $D_a = d(y_a,\mu_0)$.\end{lemma}

Since we are not proving lemma \ref{A}, we will give some heuristics as to why it is true. Recall that $p_{a,\tau_{k+1}}=\prob{\theta_{0,\tau_{k+1}} > y_a \mid \history_t}$. If $\theta_{0,\tau_{k+1}}$ were to be replaced by the sample mean $\hat{\mu}_{0,k}$, then Chernoff bound would imply that $p_{a,\tau_{k+1}} \le 1-e^{-kC}$ (nevermind the fact that $\tau_{k+1}$ is random). Furthermore, $\frac{e^{-kC}}{1-e^{-kC}} \approx e^{-kC}$ for $k\gg0$. This gives some heuristics as to why the terms inside the $\Theta$ in lemma \ref{A} should have this particular form.

\subsection{Bounding $S_2$}
\begin{lemma}\label{B}
$$\sum_{t=1}^T  \prob{A_t = a, E^{\mu}_{a,t}, \overline{E^{\theta}_{a,t}}}
\le L_a(T)+1$$
where $L_a(T) = \frac{\log T}{d(x_a,y_a)}$.
\end{lemma}
\begin{proof}First, we make sure that we have taken enough samples of $a$:
\begin{equation}\label{eq1}\sum_{t=1}^T  \prob{A_t = a, E^{\mu}_{a,t}, \overline{E^{\theta}_{a,t}}} 
    \le L_a(T) + \sum_{t=1}^T  \prob{A_t = a,  \overline{E^{\theta}_{a,t}}, E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)}.\end{equation}
    For each term in the sum in the RHS above, we condition on the history
    \begin{equation}\label{eq2}\prob{A_t = a, \overline{E^{\theta}_{a,t}}, E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)} =  \E{\prob{A_t = a,  \overline{E^{\theta}_{a,t}}, E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)\mid \history_t}}
    \end{equation}
Using the Bayes rule
    \begin{align*}&\prob{A_t = a,  \overline{E^{\theta}_{a,t}}, E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)\mid \history_t} \\= &\prob{A_t = a, \overline{E^{\theta}_{a,t}}\mid E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T), \history_t}\prob{E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)\mid \history_t}\end{align*}
    the fact that $\prob{E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)\mid \history_t}\le 1$ and (\ref{eq2}), we get
    $$ \prob{A_t = a, \overline{E^{\theta}_{a,t}}, E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T)}
    \le \E{\prob{A_t = a, \overline{E^{\theta}_{a,t}}\mid E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T), \history_t}}.$$
We now claim that 
\begin{equation}\label{claim1}\prob{A_t = a, \overline{E^{\theta}_{a,t}}\mid E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T), \history_t} \le \frac{1}{T}.\end{equation}
Given the claim, the RHS of (\ref{eq1}) $\le L_a(T) + \sum_{t+1}^T 1/T = L_a(T) +1$, which proves the lemma. So it remains to show that (\ref{claim1}) holds. Now,
\begin{align}
    &\prob{A_t = a, \overline{E^{\theta}_{a,t}}\mid E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T), \history_t}\\
    \le& \prob{ \overline{E^{\theta}_{a,t}}\mid E^{\mu}_{a,t}, N_{t-1}(a) > L_a(T), \history_t}\\
    =& \prob{ \theta_{a,t} > y_a \mid \hat{\mu}_{a,t-1} \le x_a, N_{t-1}(a) > L_a(T), \history_t} \quad \mbox{by definition of $E^{\theta}_{a,t}$ and $E^\mu_{a,t}$} \\
    =& \prob{ \theta_{a,t} > y_a \mid \underbrace{S_{a,t-1} \le x_a(N_{t-1}(a)+1), N_{t-1}(a) > L_a(T), \history_t}_{=:\bigstar}} \quad \mbox{by definition of $\hat{\mu}_{a,t-1}$} \\
    =& \prob{ Beta(S_{a,t-1}+1,N_{t-1}(a)-S_{a,t-1}+1) > y_a \mid \bigstar} \label{eq3}
\end{align}
where the last equality follows from the definition of $\theta_{a,t}$, and $Beta(\alpha,\beta) > y_a$ denotes the event that a $Beta(\alpha,\beta)$ distributed random variable is greater than $y_a$. We have a general fact about the Beta distribution:
\begin{lemma}\label{beta fact}For $\alpha'>\alpha$ and $y\in [0,1]$, we have
$$\prob{Beta(\alpha, C-\alpha)>y} \le \prob{Beta(\alpha',C-\alpha')>y}$$ 
\end{lemma}
\begin{proof}
Recall the identity
\begin{equation}\label{CDF identity}
1- \CDF_{Beta(\alpha,\beta)}(y) = \CDF_{Binom(\alpha+\beta-1,y)}(\alpha-1).
\end{equation}
Using the identity, we have
\begin{align*}\prob{Beta(\alpha, C-\alpha)>y} =& 1- \CDF_{Beta(\alpha,C-\alpha)}(y)\\
=& \CDF_{Binom(C-1,y)}(\alpha-1)\\
\le& \CDF_{Binom(C-1,y)}(\alpha'-1)\\
=& 1- \CDF_{Beta(\alpha',C-\alpha')}(y)=\prob{Beta(\alpha', C-\alpha')>y}
\end{align*}
This proves lemma \ref{beta fact}.
\end{proof}
Going back to (\ref{eq3}), we have
\begin{align}&\prob{ Beta(S_{a,t-1}+1,N_{t-1}(a)-S_{a,t-1}+1) > y_a \mid \bigstar}\\
\le& \prob{ Beta(x_a(N_{t-1}(a)+1)+1,(1-x_a)(N_{t-1}(a)+1)) > y_a \mid \bigstar}\quad \because \mbox{lemma \ref{beta fact},}\\
=&\CDF_{Binom(N_{t-1}(a)+1,y_a)}(x_a(N_{t-1}(a)+1)) \quad \because \mbox{identity (\ref{CDF identity}).}\label{eq4}\end{align}
To bound the quantity above, we need
\begin{lemma}[KL-divergence version of Chernoff-Hoeffding]\label{KL lemma}
Let $X_1,\dots, X_n$ be Bernoulli random variables. Let $p_i = \E{X_i}$, $X=\frac{1}{n} \sum_{i=1}^n X_i$, $\mu = \E{X}$. Then
\begin{align*}&\prob{X\ge \mu+\lambda} \le \exp\left(-nd(\mu+\lambda,\mu)\right),\quad \forall 0 < \lambda \le 1-\mu.\\
&\prob{X \le \mu-\lambda} \le \exp\left(-nd(\mu-\lambda,\mu)\right), \quad \forall 0 < \lambda < \mu.
\end{align*}
\end{lemma}
See the supplementary material of \cite{agrawal2013further}. Now continuing from (\ref{eq4})
\begin{align*}\CDF_{Binom(N_{t-1}(a)+1,y_a)}(x_a(N_{t-1}(a)+1)) \le &e^{-(N_{t-1}(a)+1)d(x_a,y_a)} \quad \because \mbox{lemma \ref{KL lemma}}\\
\le &e^{-L_a(T)d(x_a,y_a)} \quad \because \mbox{$N_{t-1}(a) > L_T(a)$ by $\bigstar$} \\
= &\frac{1}{T} \quad \because \mbox{definition of $L_T(a)$.}\end{align*}
This proves (\ref{claim1}) which concludes the proof of lemma \ref{B}.
\end{proof}

\subsection{Bounding $S_3$}
\begin{lemma}\label{C}
$$\sum_{t=1}^T  \prob{A_t = a, \overline{E^{\mu}_{a,t}}}\le 1 + \frac{1}{d(x_a,y_a)}.$$
\end{lemma}
\begin{proof}
Define $\tau_k$ as the time index at which action $a$ is taken for the $k$-th time and $\tau_0 = 0$.
\begin{align*}
\sum_{t=1}^T  \prob{A_t = a, \overline{E^{\mu}_{a,t}}} &\le \E{\sum_{k=0}^{T-1} \sum_{t= \tau_k +1}^{\tau_{k+1}} \ind(A_t =a) \ind(\overline{E^\mu_{a,t}})} \quad \because \tau_{T} \ge T\\
 &= \E{\sum_{k=0}^{T-1}\ind(\overline{E^\mu_{a,\tau_{k+1}}})}\\
 &\le 1+\E{\sum_{k=1}^{T-1}\ind(\overline{E^\mu_{a,\tau_{k+1}}})}\\
  &\le 1+\sum_{k=1}^{T-1} e^{-kd(x_a,\mu_a)} \quad \because \mbox{lemma \ref{KL lemma} and } \overline{E^\mu_{a,\tau_{k+1}}}= \{\hat{\mu}_{a,\tau_{k+1}} > x_a \}\\
  &\le 1+\frac{e^{-d(x_a,\mu_a)}}{1-e^{-d(x_a,\mu_a)} }\quad \because \mbox{geometric series}\\
  &\le 1+\frac{1}{d(x_a,\mu_a)}\quad \because e^x \ge 1 + x
\end{align*}
\end{proof}
\subsection{Putting it all together}
\begin{align*}\E{N_T(a)}&\le \frac{24}{\Delta_a'^2} + \sum_{j=0}^{T-1}\Theta\left(e^{-j\Delta_a'^2/2}+ \frac{1}{(j+1)\Delta_a'^2}e^{-kD_a}+ \frac{1}{e^{j\Delta_a'^2/4}-1}\right) \quad\because \mbox{lemma \ref{A}}\\
 &\quad+\frac{\log T}{d(x_a,y_a)} + 1 \quad \because \mbox{lemma \ref{B}}\\
 &\quad+ \frac{1}{d(x_a,y_a)}+ 1\quad \because \mbox{lemma \ref{C}}\\
 &\le (1+\epsilon)^2 \frac{\log T}{d(\mu_a,\mu_0)} + O\left(\frac{1}{\epsilon^2}\right) \quad \because d(x_a,y_a) = \frac{d(\mu_a,\mu_0)}{(1+\epsilon)^2} \mbox{ by construction.}
\end{align*}
\bibliography{stats710}

\end{document}


