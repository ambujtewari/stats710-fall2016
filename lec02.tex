\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage}
\usepackage{algorithm,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4}{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{2}{Sep 08, 2016}{Finite-time analysis of $\epsilon$-greedy}{Julie Ghekas}
\section{Recap from Lecture 1}
Last time, some basic principles were discussed: 

(a) basic MAB formation with stochastic (iid) rewards, 

(b) regret, as defined by 
\[
\regret_T(\la, (D_a)_{a \in \actions}) = \E {\mu_\star T - \sum_{t=1}^{T} R_t} =  \sum_{a: a \in \actions} \Delta_a \E{N_T(a)}
\]

(c) Robbins (1952) basic consistency result for the case $D_a \in \distfam := \{D: \mathbb{E}_{X \sim D} [X] < \infty\}$

\subsection{Clarifications from Lecture 1}
$\la = (\la_1, \la_2, \ldots)$ is a sequence of maps that map a new action from the previous ones: 
\[
\la_{t} : (a_1, r_1, a_2, r_2, \ldots, a_{t-1}, r_{t-1}) \rightarrow a_t
\]
Thus, $\la$ is an input to the regret formulation as it selects the action that produces the reward.

Almost all algorithms begin with a circuit of sampling each arm once, resulting in $a_1, \ldots a_K$ being predetermined, where $K$ represents the total number of arms.  That way, we ensure that each arm is sampled at least once.

The optimal regret of an algorithm in the distribution-free setting with bounded (or, more generally, subgaussian) rewards scales as $O(\sqrt{T})$.  However, in the case of algorithms that separate exploration from exploitation the rate is worse: $O(T^{2/3})$.  For some formal lower bounds that hold for any algorithm that separates the exploration and exploitation see \cite{dani2006robbing} (Section 6) and \cite{garivier2016explore}. Therefore, the minimax regret would not be achieved through separation policy, where the exploration and exploitation phases are separated.

The Robbins algorithm would still work if the rewards were not iid. All we need is the WLLN to hold for rewards. For example, we can allow for various mixing conditions that guarantee asymptotic independence, so this conditions would still permit a consistency proof to go through.

\section{Algorithm Design Principles/Heuristics}
\subsection{Robbins (1952) strategy}
One high level strategy is the exploration/exploitation discussed in Lecture 1.  This strategy has different names depending on the field.  Typically, electrical engineering calls this algorithm ``certainty equivalence with forcing'', while computer scientists use the term ``phased exploration with greedy exploitation''.  While there may be some differences at the detailed level between these two phrases, they are closely related to each other.

Important characteristics of Robbins' strategy include that we have to be looking at an infinite sequence of time and that there is a vanishing density of any point being an exploration time.  Exploration times have to be pre-selected before any process begins. 

\subsection{$\epsilon$-greedy}
This is similar to Robbins' method but the exploration steps are probabilistically selected rather than pre-determined.

In this randomization-based algorithm, a coin is flipped at each round.  If it comes up heads, you explore, i.e. take a random action, usually based on a constant/uniform selection of the arms.  If it comes up tails, you behave ``greedily'', i.e. go with the maximum sample mean at that point.  One important point to note is that the coin's probability can change with time.  A lot of papers use constant $\epsilon$.  A potential project idea is to understand mathematically/theoretically the selection of $\epsilon$ especially in the presence of non-stationarity of reward distributions.  

\subsection{Index-based algorithms}
One example of an index-based algorithm is the upper confidence bound (UCB) that we will study later.  In these algorithms, the exploration phase is not separated from the exploitation phase.  Arms that are not sampled enough are giving an ``exploration bonus'' to entice the algorithm to sample more often from those arms.

\subsection{Thompson/posterior sampling}

This method follows the principles of Bayesian methodology which is best illustrated in the parametric case.

For a vector of parameters $\overrightarrow {\theta} = (\theta_0, \ldots, \theta_{K-1})$, a prior distribution is assigned to $\overrightarrow {\theta} : p(\overrightarrow{\theta})$.  For $\overrightarrow{\theta}$, there is an optimal distribution.  We can sample from the Bayesian posterior to obtain estimates of the optimal parameter: 
\[
\ P(\overrightarrow{\theta} \mid \history_{t}) \propto p(\overrightarrow{\theta}) P(\history_{t} \mid \overrightarrow{\theta})
\]

In this case, rewards can be designed under a parametric family but can be analyzed outside of these parametric assumptions, landing in more optimistic range of the parameter space.

There are some who are trying to unify the index-based and Thompson algorithms, which could work into another potential project idea.

\subsection{Softmax/Boltzmann exploration}
This method softens the greedy component of the $\epsilon$-greedy algorithm.  Instead of pursuing the max, the softmax is used.  In this case, $\mathbb{P}(a_{t}=a) \propto \frac{\exp(\hat{\mu}_{t-1,a})}{\eta}$, where $\hat{\mu}_{t-1,a}$ represents the estimate of the mean reward of $a$ and $\eta$ represents the ``temperature'' (this terminology comes from statistical physics).  As $\eta\rightarrow 0$, the probability will concentrate on the greedy actions: i.e., those with maximum estimated reward, but this method is a way of softening the selection of the max.

There is no known finite time analysis of this to the best of our knowledge.  A variant of Boltzmann exploration changed the softmax to the softmix, which combines softmax with another layer of uniform sampling and behaves similarly to an $\epsilon$-greedy algorithm mixed with softmax \cite{cesabianchi1998finite}.

\subsection{Bootstrap}
Also called the ``poor man's Bayes'', the bootstrap can be used by subsampling to create a number of parameter estimates.  We use the histogram created from the subsampling parameter estimates to get an idea of the parameter distribution.  We are unlikely to discuss this method in class but see \cite{eckles2014thompson,baransi2014sub,osband2015bootstrapped} for related ideas.
 
\section{Finite-time regret analysis of $\epsilon$-greedy}

For this section, we assume that all rewards are bounded, say in $[0,1]$.  After sampling each arm once, the following algorithm is executed:

\begin{algorithmic}[1]
\FOR{$t=1$ to $T$}
\STATE Learner selects random arm $a \in \actions$ with probability $\epsilon_{t}$
\STATE Learner selects $a_t \in \actions$ where $a_t := \argmax_{a \in \actions} \hat{\mu}_{t-1, a}$ with remaining probability $1-\epsilon_{t}$.
\ENDFOR
\end{algorithmic}

%Under this setting, $a_t$ has probability $1-\epsilon_t(\frac{K-1}{K})$ of being selected, while all other actions have probability $\frac{\epsilon_t}{K}$ of being selected.

The coin flips used to decide whether the learning algorithm explores by taking a random action are assumed to occur independent of nature.

\subsection{Regret bound}

\begin{theorem}
Suppose $D_a$ has support in $[0, 1], \forall a \in \actions$.  Define $\epsilon_t = \min\{1, \frac{cK}{d^2t}\}$, where $K$ is the number of actions possible, $t$ is the time, $c > 5$, and $0 \le d \le min_{a:\mu_a < \mu_\star} \Delta_a$   Run $\epsilon_t$-greedy algorithm under these constraints, and fix a suboptimal arm $a \in \actions$.  Then the probability that $\epsilon_t$-greedy chooses $a$ at $t$, for $t \ge \frac{cK}{d}$, is at most $\frac{c}{d^2t}+o(\frac{1}{t})$.  That is to say, jointly over the algorithm and nature for every $t$ large enough, 
\[
    \ \mathbb{P}(A_t = a) \le \frac{c}{d^2t} + o \left(\frac{1}{t} \right)
\]
\end{theorem}

One question associated with this particular theorem is how to select $d$.  One hopes to select the largest $d$ possible, but $d$ also must be less than the unknown gap between $a$ and the optimal arm.  
The algorithm is sensitive to the choice of $d$. If $d=0$, then the bound blows up and becomes useless (especially if the reward is assumed to be bounded).

The rate of growth of the regret in this case is $\log(t)$.

\subsection{Corollary}
\[
\ \regret_T(\epsilon_t\text{-greedy}, (D_a)_{a \in \actions}) \le \frac{c}{d^2} \log(T) (\sum_{a \in \actions} \Delta_a)
\]
Here, $\sum_{a \in \actions} \Delta_a$ represents the suboptimality gap.  One does not need to condition the addition on the fact that $\mu_a < \mu_\star$ as before, since if $\mu_a = \mu_\star$, then $\Delta_a = 0$.  

We need two facts (concentration inequalities) before we begin the proof (see next lecture).

\subsection{Fact 1: Hoeffding-Azuma concentration inequality}
Let $X_1, X_2, \ldots$ be random variables with range $[0, 1]$ and $\mathbb{E}(X_t | X_{1:t-1}) = \mu$, where $X_{1:t-1}$ represents $X_1, X_2, \ldots, X_{t-1}$.  Then 
\[
\ \forall \delta > 0, \prob{\frac{\sum_{i=1}^t X_i}{t} - \mu \ge \delta} \le \exp(-2t\delta^2)
\]
 and 
 \[
 \ \forall \delta >0, \prob{\mu -\frac{\sum_{i=1}^t X_i}{t} \ge \delta} \le \exp(-2t\delta^2)
 \]

Our random variables in this case could be iid, although this is not a requirement for this fact.

\subsection{Fact 2: Bernstein concentration inequality}
As before, let $X_1, X_2, \ldots$ be random variables with range $[0,1]$ and $\sum_{i=1}^t \mathrm{Var}(X_i \mid X_{1:i-1}) = \sigma_t^2$.  Then 
\[
\ \forall \delta>0, \prob{\sum_{i=1}^t X_i - \E{\sum_{i=1}^t X_i} \ge \delta} \le \exp \left( \frac{-\delta^2/2}{\sigma_t^2+ \delta/2} \right)
\]
and 
\[
\ \forall \delta >0, \prob{ \E{\sum_{i=1}^t X_i} - \sum_{i=1}^t X_i \ge \delta} \le \exp \left( \frac{-\delta^2/2}{\sigma_t^2+ \delta/2} \right)
\]

The random variables we will apply Bernstein's inequality to will be independent but not identically distributed.

\bibliography{stats710}
\end{document}


