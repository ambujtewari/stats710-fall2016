\documentclass[11pt]{article}
\usepackage{url,amsmath,setspace,amssymb,amsthm,fullpage,algorithmic}

% Scribe template modified from original created by UC Berkeley's EECS department

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf STATS 710 -- Seq. Dec. Making with mHealth Applications} \hfill #2

       {\centering \Large #5

       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\scribe}[4]{\heading{#1}{#2}{Instructors:
Susan Murphy and Ambuj Tewari}{Scribe: #4}{Lecture #1: #3}}

\input{macros}

\bibliographystyle{alpha}

\begin{document}
\scribe{10}{Oct 11, 2016}{Contextual Bandits}{Peng Liao}

\section{Contextual Bandits Problem}

Contextual bandit problems are also known as ``bandit problems with side-information", ``bandit problems with covariates", and ``associative reinforcement learning".

The first paper on contextual bandit was written by Woodroofe in 1979 \cite{woodroofe1979one}. The term ``Contexual Bandit" was coined by Langford and Zhang in 2008 \cite{langford2008epoch}. 
The general flow of information for a multi-armed bandit with contexts drawn from a set $\mathcal{X}$ is:

\begin{algorithmic}[1]
\FOR{$t=1$ to $T$}
\STATE Learner sees $X_t \in \mathcal{X}$
\STATE Learner selects action $A_t \in \actions$
\STATE Learner receives reward $R_t = R^{A_t}_t$
\ENDFOR
\end{algorithmic}

\section{Definition of Regret}

There are different ways to define the regret in contextual bandit setting. In the stochastic setting,  we assume that the context-reward pair $(X_t, R_t) = \{X_t, R_t^a, a \in \actions \}$ are i.i.d. across time.  Given a joint distribution $D$ on $(X, R) = \{X, R^a, a \in \actions \}$ and a class of (deterministic) policies $\Pi \subseteq \actions^\mathcal{X}$, the expected regret of learning algorithm $\la$ is defined as
\begin{equation*}
\regret_T(\la, D, \Pi) 
:= \sup_{\pi \in \Pi} 
\E{ \sum_{t=1}^{T} R_{t}^{\pi(X_t)} - \sum_{t=1}^{T} R_t^{A_t}  }
= T V(\pi^*)  - \E{ \sum_{t=1}^{T} R_t^{A_t}  }
\end{equation*}
where $V(\pi) := \mathbb{E}_{(X, R) \sim D} \big[ R^{\pi(X)} \big]$ is the value function and $\pi^* = \argmax_{\pi \in \Pi} V(\pi)$ is the optimal policy of the class $\Pi$.  One can also generalize the regret competing with stochastic policies.   The learner could have access to the policy class he is competing against, but not the underlying distribution. 


As opposed to the stochastic MAB,  here the notation of regret is with respect to the best policy in a class of policies rather than the best arm.  Even though it may seem like contextual bandits is an easier problem than MAB (we have additional information), it is in fact a harder problem from the regret perspective (we want to compete against policies, not pulling the same optimal arm). 


There are two main approaches (or strategies)  to solve the contextual bandits problem, depending on the assumptions one is willing to make.     In the first approach, one does not want to make any simplifying assumption on the underlying  distribution $D$ of $(X, R)$, but only wants to compete against a relatively simple class of policies.  This is similar to a classification problem: the classifier now becomes policy and we are interested in quantifying the  risk w.r.t.  the best classifier in the model.    On the other hand, if one is comfortable putting some structure on the distribution $D$, e.g. assuming a linear model $\E{R|X, A} = \beta'f(X, A)$, we could then compete with the optimal policy over all possible policies.   This is the second approach  and is related to a regression problem in the sense that the learner needs to estimate the mean function $\E{R|X, A}$ and use it to take actions. 





\section{Reducing to  MAB}

When the context space $\mathcal{X}$ is finite (and small), one natural idea is to reduce the contextual bandits to a multiarm bandits problem. To be specific, suppose we have assess to a MAB algorithm with (non-stochastic) regret bound of $f(K) g(T)$, e.g. $f(K) = \sqrt{K\log K}$ and $g(T) = \sqrt{T}$ in EXP3 when rewards are within $[0, 1]$,  and run $|\mathcal{X}|$ copies of this algorithm, one for each element of $\mathcal{X}$. The expected regret against all policies can be bounded by
\begin{align}
& \max_{\pi: \mathcal{X} \rightarrow \actions} \E{ \sum_{t=1}^{T} R_{t}^{\pi(X_t)} - \sum_{t=1}^{T} R_t^{A_t}  } \notag\\
& = \max_{\pi: \mathcal{X} \rightarrow \actions} \E{ \sum_{x \in \mathcal{X}} \sum_{t: X_t = x} (R_{t}^{\pi(X_t)} - R_t^{A_t})  } \notag\\
& = \max_{\pi: \mathcal{X} \rightarrow \actions} \sum_{x \in \mathcal{X}} \E{  \sum_{t: X_t = x} R_{t}^{\pi(x)} - R_t^{A_t}  }\notag\\
& = \sum_{x \in \mathcal{X}} \max_{a \in \actions}\E{  \sum_{t: X_t = x} R_{t}^{a} - R_t^{A_t}  }\label{eq0}\\
& \leq \sum_{x \in \mathcal{X}} f(K) g(T_x)   \label{eq1}
\end{align}
where $T_x = \sum_{t=1}^{T} \mathbf{1}_{\{X_t = x\}}$ is the number of rounds that we see context $x$ in $T$ round, the equality (\ref{eq0}) holds because the maximization is over all policies and thus we can exchange the sum and max, and we use the regret bound of the underlying MAB algorithm to get inequality (\ref{eq1}). Note however that MAB copy for context $x$ runs for a random number $T_x$ of time steps.

Now assuming that $g(T)$ is concave, then applying Jensen's inequality gives
\begin{align*}
\sum_{x \in \mathcal{X}}  f(K) g(T_x)  = f(K) \sum_{x \in \mathcal{X}} g(T_x) & = f(K) |\mathcal{X}| \sum_{x \in \mathcal{X}} \frac{1}{|\mathcal{X}|} g(T_x) \\
& \leq f(K) |\mathcal{X}| g\left(\sum_x  \frac{1}{|\mathcal{X}|} T_x \right)\\
& =  f(K) |\mathcal{X}| g\left(\frac{T}{|\mathcal{X}|}\right)
\end{align*}
Thus in the case of EXP3, we get $\sqrt{K \log K} \sqrt{T |\mathcal{X}|}$. One can show that the factor $\sqrt{|\mathcal{X}|}$ can not be improved by using the lower bound of MAB and thus the above bound is tight (up to the $\sqrt{\log K}$ factor).
\newpage
\bibliography{stats710}
\end{document}


